{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$ \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mQa._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.Qa\n",
    "import Qa._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scala: Spark Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Spark?\n",
    "Apache Spark is a computation and processing library. It distributes data over a large number of nodes, and for performing paraellel computations over that cluster.\n",
    "\n",
    "### Spark Computation\n",
    "\n",
    "\n",
    "Spark operations concern Resilient Distributed Data (RDD) objects: representations of the data partitioned across multiple nodes, able to be operated on in parallel.\n",
    "\n",
    "In Spark 1, programmers work with RDDs directly. In Spark 2, the DataFrame and SQL API is strongly preferred -- but RDDs are still available. Since quite a lot of legacy code still exists, and RDDs still have their use in Spark 2, we start with RDDs. However Databricks (the authors) strongly recommends DataFrames for new projects. (And \"Datasets\" in Scala which are strongly-typed DataFrames).  \n",
    "\n",
    "The general approach with RDDs is:\n",
    "\n",
    "1. Create an RDD representation of the data set, distributing data across the cluster\n",
    "2. Perform a **transformation** on the RDD representation, producing a new distributed dataset\n",
    "3. Perform an **action** to extract the final result from the cluster, in a non-distributed format \n",
    "\n",
    "Transformations are not computed until an action takes place. \n",
    "\n",
    "### Spark Languages\n",
    "\n",
    "The native language for Spark is Scala. This is for good reason - most operations associated with Spark are transformations and actions on RDDs, and functional programming lends itself well to these types of operation.\n",
    "\n",
    "A Spark program would consists of a sequence of mappings applied one after the other on an initial RDD.\n",
    "\n",
    "However, Spark is not restricted to Scala, it also provides APIs to most of the popular Data Science languages such as Python and R.\n",
    "\n",
    "\n",
    "### Spark Clusters\n",
    "\n",
    "\n",
    "Spark can be setup on any cluster, including Hadoop, in which case it would be integrated with Yet Another Resource Negotiator (YARN).\n",
    "\n",
    "The main difference between a Spark RDD and a file on HDFS is that the Spark RDD lives in the memory of each of the nodes, while a file on HDFS lives on their respective hard drives. \n",
    "\n",
    "Hence, while Hadoop MapReduce always reads and writes to files, operations on Spark RDDs are performed *in memory*, making Spark - in theory - **several orders of magnitude faster**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres2\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@2373eca9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres3\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m]] = \u001b[33mMap\u001b[39m(\n",
       "  \u001b[32m\"name\"\u001b[39m -> \u001b[33mArray\u001b[39m(\u001b[32m\"Michael\"\u001b[39m, \u001b[32m\"Kunal\"\u001b[39m),\n",
       "  \u001b[32m\"age\"\u001b[39m -> \u001b[33mArray\u001b[39m(\u001b[32m\"30\"\u001b[39m, \u001b[32m\"20\"\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Map(\n",
    "    \"name\" -> Array(\"Michael\", \"Kunal\"),\n",
    "    \"age\" -> Array(\"30\", \"20\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [age: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.range(18, 30).toDF(\"age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transformation Pipeline vs. Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transform = [age: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val transform = df.where(\"age < 25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Actions cause results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result = 7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = transform.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infering Schema (Read-Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crime = [M: int, So: int ... 14 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[M: int, So: int ... 14 more fields]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val crime = spark\n",
    "    .read\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(\"crime.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(M, So, Ed, Po1, Po2, LF, M.F, Pop, NW, U1, U2, GDP, Ineq, Prob, Time, y)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121,0,110,118,115,547,964,25,44,84,29,689,126,0.034201,20.9995,682]\n",
      "[130,0,116,128,128,536,934,51,24,78,34,627,135,0.019099,24.9008,750]\n"
     ]
    }
   ],
   "source": [
    "crime.sort(\"Ineq\").take(2).foreach { println }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain & Spark Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd4.sc:1: not found: value crime\n",
      "val res4 = crime.sort(\"Ineq\").explain()\n",
      "           ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "crime.sort(\"Ineq\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL vs DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Register table with spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.createOrReplaceTempView(\"crime_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use table on `spark` session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SothernState|        Inequality|\n",
      "+------------+------------------+\n",
      "|           0|173.09677419354838|\n",
      "|           1|             234.5|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT So AS SothernState, AVG(Ineq) AS Inequality\n",
    "    FROM crime_data\n",
    "    GROUP BY So\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With dataframe api, no need to register anything\n",
    "* Use methods directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| So|         avg(Ineq)|\n",
      "+---+------------------+\n",
      "|  0|173.09677419354838|\n",
      "|  1|             234.5|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crime.groupBy(\"So\").avg(\"Ineq\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      "Bye!\n",
      "Hi\n",
      "Bye!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject\u001b[39m \u001b[36mPerson\u001b[39m\n",
       "\u001b[36mres10_1\u001b[39m: \u001b[32mPerson\u001b[39m.type = ammonite.$sess.cmd10$Helper$Person$@7819f772"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object Person {\n",
    "    def say() = {\n",
    "        println(\"Hi\")\n",
    "        Person\n",
    "    }\n",
    "    def bye() = {\n",
    "        println(\"Bye!\")\n",
    "        Person\n",
    "    }\n",
    "}\n",
    "\n",
    "Person\n",
    "    .say()\n",
    "    .bye()\n",
    "    .say()\n",
    "    .bye()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "| So| EducationAverage|\n",
      "+---+-----------------+\n",
      "|  0|111.2258064516129|\n",
      "+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.desc\n",
    "\n",
    "crime\n",
    "  .groupBy(\"So\")\n",
    "  .avg(\"Ed\")\n",
    "  .withColumnRenamed(\"avg(Ed)\", \"EducationAverage\")\n",
    "  .sort(desc(\"EducationAverage\"))\n",
    "  .limit(1)\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.Logger\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mobject\u001b[39m \u001b[36mMyLib\u001b[39m\n",
       "defined \u001b[32mobject\u001b[39m \u001b[36mSparkApplication\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.Logger\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "\n",
    "object MyLib extends Serializable {\n",
    "  @transient lazy val logger = Logger.getLogger(getClass.getName)\n",
    "\n",
    "  def userDefinedFn(input: String): String = {\n",
    "    logger.info(input)\n",
    "    input.toUpperCase\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "object SparkApplication extends Serializable {\n",
    "    import spark.implicits._\n",
    "    def main(args: Array[String]) = {\n",
    "\n",
    "    val spark = SparkSession\n",
    "        .builder()\n",
    "        .appName(\"Spark Application Example\")\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark.udf.register(\"userDefinedFn\", MyLib.userDefinedFn _)\n",
    "\n",
    "    \n",
    "    val authorsDF = spark\n",
    "        .sparkContext\n",
    "        .parallelize(Array(\"sample text\", \"some more\"))\n",
    "        .toDF(\"output\")\n",
    "        .selectExpr(\"split(output, ' ') as values\")\n",
    "        .selectExpr(\"userDefinedFn(values[0]) as first\", \"values[1] as second\")\n",
    "        .show()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/07/25 12:40:55 INFO cmd2$Helper$MyLib$: sample\n",
      "19/07/25 12:40:55 INFO cmd2$Helper$MyLib$: some\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| first|second|\n",
      "+------+------+\n",
      "|SAMPLE|  text|\n",
      "|  SOME|  more|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SparkApplication.main(Array())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
