{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$ \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mQa._\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.Qa\n",
    "import Qa._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "* spark is a library for distributed computation\n",
    "    * distributed = data may exist on multiple machines\n",
    "    * computation is performed by multiple machines\n",
    "    * result is \"collected\" onto a final *driver* machine\n",
    "    \n",
    "    \n",
    "\n",
    "* driver = the computer which issues the query & collects result\n",
    "* executors (/workers) = the machines which perform the calc, and have the data\n",
    "\n",
    "\n",
    "\n",
    "* RDD = resilitant distributed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scala Spark: RDDs and DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Context Handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For RDDs the central handler of a Spark session is the `SparkContext` object.\n",
    "\n",
    "A `SparkContext` object is your handler for calling Spark functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@7f5bd7c0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to create RDD objects:\n",
    "1. From list or arrays defined within the program\n",
    "2. By reading from normal files\n",
    "3. Reading from Hadoop HDFS\n",
    "4. From the output of Hive queries\n",
    "5. From the output of normal databases queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we create RDDs from lists and arrays directly, using the `SparkContext.parallelize()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[0] at parallelize at cmd6.sc:1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(Array(1,2,3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at what the object we have created looks like, we see that it is different to a core scala collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres7\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[0] at parallelize at cmd6.sc:1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mPerson\u001b[39m\n",
       "\u001b[36mds\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mPerson\u001b[39m] = ParallelCollectionRDD[1] at parallelize at cmd8.sc:3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Person(val name: String)\n",
    "\n",
    "val ds = sc.parallelize(Array(Person(\"Michael\"), Person(\"Michael\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres9\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = MapPartitionsRDD[2] at map at cmd9.sc:1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.map  { _.name }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can easily extract the information we stored as RDD by using the `collect()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres10\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"Michael\"\u001b[39m, \u001b[32m\"Michael\"\u001b[39m)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.map {_.name}.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres11\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This *localizes* the dataset: it was distributed in an RDD, now it's here in the memory of this machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For standard files we can use the `textfile()` method to read data in from a specified filepath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd_file\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = test.csv MapPartitionsRDD[5] at textFile at cmd12.sc:1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_file = sc.textFile(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `take()` allows us to specify how many lines we wish to see from the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres14\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"id,vendor_id,pickup_datetime,passenger_count,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,store_and_fwd_flag\"\u001b[39m,\n",
       "  \u001b[32m\"id3004672,1,30/06/2016 23:59,1,-73.98812866,40.73202896,-73.99017334,40.75667953,N\"\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_file.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For HDFS files we can use the same function, but a different protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// val rdd_hdfs = sc.textFile('hdfs:///path_to_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD objects are *lazy* data structures, in that they only contain the logic to obtain results. They will generally only output results *until* an action method is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have an RDD. All the methods below are action methods, in that they all produce output in a non-distributed format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mr\u001b[39m: \u001b[32mRandom\u001b[39m = scala.util.Random@7547162c\n",
       "\u001b[36mnormals\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m9\u001b[39m,\n",
       "  \u001b[32m9\u001b[39m,\n",
       "  \u001b[32m9\u001b[39m,\n",
       "  \u001b[32m11\u001b[39m,\n",
       "  \u001b[32m11\u001b[39m,\n",
       "  \u001b[32m9\u001b[39m,\n",
       "  \u001b[32m6\u001b[39m,\n",
       "  \u001b[32m11\u001b[39m,\n",
       "  \u001b[32m6\u001b[39m,\n",
       "  \u001b[32m10\u001b[39m,\n",
       "  \u001b[32m9\u001b[39m,\n",
       "  \u001b[32m11\u001b[39m,\n",
       "  \u001b[32m11\u001b[39m,\n",
       "  \u001b[32m9\u001b[39m,\n",
       "  \u001b[32m13\u001b[39m,\n",
       "  \u001b[32m12\u001b[39m,\n",
       "  \u001b[32m10\u001b[39m,\n",
       "  \u001b[32m10\u001b[39m,\n",
       "  \u001b[32m9\u001b[39m,\n",
       "  \u001b[32m8\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Random\n",
    "\n",
    "val r = new Random()\n",
    "val normals = Array.fill(20)(r.nextGaussian).map(2 * _ + 10).map(_.toInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[6] at parallelize at cmd16.sc:1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres17\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m9\u001b[39m,\n",
       "  \u001b[32m9\u001b[39m,\n",
       "  \u001b[32m9\u001b[39m,\n",
       "  \u001b[32m11\u001b[39m,\n",
       "  \u001b[32m11\u001b[39m,\n",
       "  \u001b[32m9\u001b[39m,\n",
       "  \u001b[32m6\u001b[39m,\n",
       "  \u001b[32m11\u001b[39m,\n",
       "  \u001b[32m6\u001b[39m,\n",
       "  \u001b[32m10\u001b[39m,\n",
       "  \u001b[32m9\u001b[39m,\n",
       "  \u001b[32m11\u001b[39m,\n",
       "  \u001b[32m11\u001b[39m,\n",
       "  \u001b[32m9\u001b[39m,\n",
       "  \u001b[32m13\u001b[39m,\n",
       "  \u001b[32m12\u001b[39m,\n",
       "  \u001b[32m10\u001b[39m,\n",
       "  \u001b[32m10\u001b[39m,\n",
       "  \u001b[32m9\u001b[39m,\n",
       "  \u001b[32m8\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres28\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m9\u001b[39m)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres27\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m13\u001b[39m)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.top(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres20\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m20L\u001b[39m"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to understand the distribution of values within the dataset, and so we use the function `countByValue()` to bucket and count each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres33\u001b[39m: \u001b[32mcollection\u001b[39m.\u001b[32mMap\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mLong\u001b[39m] = \u001b[33mMap\u001b[39m(\u001b[32m\"Michael\"\u001b[39m -> \u001b[32m2L\u001b[39m)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.map{_.name}.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mhist\u001b[39m: \u001b[32mcollection\u001b[39m.\u001b[32mMap\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mLong\u001b[39m] = \u001b[33mMap\u001b[39m(\n",
       "  \u001b[32m10\u001b[39m -> \u001b[32m3L\u001b[39m,\n",
       "  \u001b[32m6\u001b[39m -> \u001b[32m2L\u001b[39m,\n",
       "  \u001b[32m9\u001b[39m -> \u001b[32m7L\u001b[39m,\n",
       "  \u001b[32m13\u001b[39m -> \u001b[32m1L\u001b[39m,\n",
       "  \u001b[32m12\u001b[39m -> \u001b[32m1L\u001b[39m,\n",
       "  \u001b[32m11\u001b[39m -> \u001b[32m5L\u001b[39m,\n",
       "  \u001b[32m8\u001b[39m -> \u001b[32m1L\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hist = rdd.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mx\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mVector\u001b[39m(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m, \u001b[32m9\u001b[39m, \u001b[32m13\u001b[39m, \u001b[32m12\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m8\u001b[39m)\n",
       "\u001b[36my\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mLong\u001b[39m] = \u001b[33mStream\u001b[39m(\u001b[32m3L\u001b[39m, \u001b[32m2L\u001b[39m, \u001b[32m7L\u001b[39m, \u001b[32m1L\u001b[39m, \u001b[32m1L\u001b[39m, \u001b[32m5L\u001b[39m, \u001b[32m1L\u001b[39m)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = hist.keySet.toSeq\n",
    "val y = hist.values.toSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly._, plotly.Almond._\n",
    "\n",
    "plot(Seq(Bar(x,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below methods all perform reductive operations upon the RDD structure, in that the output of each is a single, unitary object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres34\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m193.0\u001b[39m"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres35\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m6\u001b[39m"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres36\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m13\u001b[39m"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres37\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m9.65\u001b[39m"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres38\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m541\u001b[39m"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce((t, e) => t + 2 * e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres39\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m1057596416\u001b[39m"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.fold(1)(_ * _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"column-count: 4\"> aggregate<br />barrier<br />cache<br />cartesian<br />checkpoint<br />checkpointData<br />clearDependencies<br />coalesce<br />collect<br />collectPartitions<br />compute<br />computeOrReadCheckpoint<br />conf<br />context<br />count<br />countApprox<br />countApproxDistinct<br />countByValue<br />countByValueApprox<br />creationSite<br />dependencies<br />distinct<br />doCheckpoint<br />doubleRDDToDoubleRDDFunctions<br />elementClassTag<br />equals<br />filter<br />first<br />firstParent<br />flatMap<br />fold<br />foreach<br />foreachPartition<br />getCheckpointFile<br />getClass<br />getCreationSite<br />getDependencies<br />getNarrowAncestors<br />getNumPartitions<br />getOrCompute<br />getOutputDeterministicLevel<br />getPartitions<br />getPreferredLocations<br />getStorageLevel<br />glom<br />groupBy<br />hashCode<br />id<br />initializeLogIfNecessary<br />intersection<br />isBarrier<br />isBarrier_<br />isCheckpointed<br />isCheckpointedAndMaterialized<br />isEmpty<br />isLocallyCheckpointed<br />isReliablyCheckpointed<br />isTraceEnabled<br />iterator<br />keyBy<br />localCheckpoint<br />log<br />logDebug<br />logError<br />logInfo<br />logName<br />logTrace<br />logWarning<br />map<br />mapPartitions<br />mapPartitionsInternal<br />mapPartitionsWithIndex<br />mapPartitionsWithIndexInternal<br />markCheckpointed<br />max<br />min<br />name<br />notify<br />notifyAll<br />numericRDDToDoubleRDDFunctions<br />outputDeterministicLevel<br />parent<br />partitioner<br />partitions<br />persist<br />pipe<br />preferredLocations<br />randomSampleWithRange<br />randomSplit<br />rddToAsyncRDDActions<br />rddToOrderedRDDFunctions<br />rddToPairRDDFunctions<br />rddToSequenceFileRDDFunctions<br />reduce<br />repartition<br />retag<br />sample<br />saveAsObjectFile<br />saveAsTextFile<br />scope<br />setName<br />slice<br />sortBy<br />sparkContext<br />subtract<br />take<br />takeOrdered<br />takeSample<br />toDebugString<br />toJavaRDD<br />toLocalIterator<br />toString<br />top<br />treeAggregate<br />treeReduce<br />union<br />unpersist<br />wait<br />withScope<br />zip<br />zipPartitions<br />zipWithIndex<br />zipWithUniqueId </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Qa.dir(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations on the RDD are given as functions to .map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres41\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m9\u001b[39m, \u001b[32m9\u001b[39m, \u001b[32m9\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m11\u001b[39m)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect().slice(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres42\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m18\u001b[39m, \u001b[32m18\u001b[39m, \u001b[32m18\u001b[39m, \u001b[32m22\u001b[39m, \u001b[32m22\u001b[39m)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map( 2 * _ ).collect().slice(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres44\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m8.9\u001b[39m, \u001b[32m9.1\u001b[39m, \u001b[32m8.9\u001b[39m, \u001b[32m9.1\u001b[39m, \u001b[32m8.9\u001b[39m)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(x => Array(x-0.1, x+0.1)).collect().slice(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres45\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m6\u001b[39m, \u001b[32m6\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m12\u001b[39m, \u001b[32m10\u001b[39m)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(_ % 2 == 0).collect().slice(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres46\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m6\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m12\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m13\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m9\u001b[39m)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres47\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m9\u001b[39m, \u001b[32m9\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m6\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m9\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m9\u001b[39m, \u001b[32m9\u001b[39m, \u001b[32m8\u001b[39m)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// take random samples; sample half of the rdd, with values not replaced\n",
    "rdd.sample(false, 0.5).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below example, we demonstrate a number of Spark functions by loading in a dataset and extracting information from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrsp\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = test.csv MapPartitionsRDD[41] at textFile at cmd50.sc:1\n",
       "\u001b[36mparse\u001b[39m: \u001b[32mString\u001b[39m => \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = ammonite.$sess.cmd50$Helper$$Lambda$4762/1089318564@52e1f602\n",
       "\u001b[36mheader\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"id\"\u001b[39m,\n",
       "  \u001b[32m\"vendor_id\"\u001b[39m,\n",
       "  \u001b[32m\"pickup_datetime\"\u001b[39m,\n",
       "  \u001b[32m\"passenger_count\"\u001b[39m,\n",
       "  \u001b[32m\"pickup_longitude\"\u001b[39m,\n",
       "  \u001b[32m\"pickup_latitude\"\u001b[39m,\n",
       "  \u001b[32m\"dropoff_longitude\"\u001b[39m,\n",
       "  \u001b[32m\"dropoff_latitude\"\u001b[39m,\n",
       "  \u001b[32m\"store_and_fwd_flag\"\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rsp = sc.textFile(\"test.csv\")\n",
    "\n",
    "val parse = (line: String) => line.trim().split(\",\")\n",
    "    \n",
    "val header = parse(rsp.take(1).head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading in the file, we look to isolate the third column. To do this, we write a lambda function which discretises the input and isolates the column. We then apply this to the RDD by using the `map()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mresults\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m]] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    \u001b[32m\"id\"\u001b[39m,\n",
       "    \u001b[32m\"vendor_id\"\u001b[39m,\n",
       "    \u001b[32m\"pickup_datetime\"\u001b[39m,\n",
       "    \u001b[32m\"passenger_count\"\u001b[39m,\n",
       "    \u001b[32m\"pickup_longitude\"\u001b[39m,\n",
       "    \u001b[32m\"pickup_latitude\"\u001b[39m,\n",
       "    \u001b[32m\"dropoff_longitude\"\u001b[39m,\n",
       "    \u001b[32m\"dropoff_latitude\"\u001b[39m,\n",
       "    \u001b[32m\"store_and_fwd_flag\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    \u001b[32m\"id3004672\"\u001b[39m,\n",
       "    \u001b[32m\"1\"\u001b[39m,\n",
       "    \u001b[32m\"30/06/2016 23:59\"\u001b[39m,\n",
       "    \u001b[32m\"1\"\u001b[39m,\n",
       "    \u001b[32m\"-73.98812866\"\u001b[39m,\n",
       "    \u001b[32m\"40.73202896\"\u001b[39m,\n",
       "    \u001b[32m\"-73.99017334\"\u001b[39m,\n",
       "    \u001b[32m\"40.75667953\"\u001b[39m,\n",
       "    \u001b[32m\"N\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    \u001b[32m\"id3505355\"\u001b[39m,\n",
       "    \u001b[32m\"1\"\u001b[39m,\n",
       "    \u001b[32m\"30/06/2016 23:59\"\u001b[39m,\n",
       "    \u001b[32m\"1\"\u001b[39m,\n",
       "    \u001b[32m\"-73.96420288\"\u001b[39m,\n",
       "    \u001b[32m\"40.67999268\"\u001b[39m,\n",
       "    \u001b[32m\"-73.95980835\"\u001b[39m,\n",
       "    \u001b[32m\"40.65540314\"\u001b[39m,\n",
       "    \u001b[32m\"N\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    \u001b[32m\"id1217141\"\u001b[39m,\n",
       "    \u001b[32m\"1\"\u001b[39m,\n",
       "    \u001b[32m\"30/06/2016 23:59\"\u001b[39m,\n",
       "    \u001b[32m\"1\"\u001b[39m,\n",
       "..."
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results = rsp.map(parse).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres67\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"30/06/2016 23:59\"\u001b[39m"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results(3)(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key-Value RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key-value RDDs contain pairs of values for each item in the collection, again, distributed across multiple nodes.\n",
    "\n",
    "The first element of each tuple is called the \"key\", and the second the \"value\".\n",
    "\n",
    "**Note**: These should not be confused with a  Map. Key-value RDDs are permitted repeat/duplicate keys, whereas Maps are not.\n",
    "\n",
    "Below, we instantiate a key-value RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mkv\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = ParallelCollectionRDD[57] at parallelize at cmd68.sc:1\n",
       "\u001b[36mres68_1\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, \u001b[32m5\u001b[39m),\n",
       "  (\u001b[32m\"Bob\"\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, \u001b[32m3\u001b[39m),\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, \u001b[32m1\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kv = sc.parallelize(\n",
    "    Array(\n",
    "        (\"Alice\", 5),\n",
    "        (\"Bob\", 2),\n",
    "        (\"Charlie\", 3),\n",
    "        (\"Alice\", 2),\n",
    "        (\"Charlie\", 1)\n",
    "    )\n",
    ")\n",
    "kv.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of actions we can perform which are specific to key-value RDD structures. We demonstrate a number of these below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to consolidate those keys which are not unique, and combine the values, then we can use the `reduceByKey()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres69\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m((\u001b[32m\"Alice\"\u001b[39m, \u001b[32m7\u001b[39m), (\u001b[32m\"Charlie\"\u001b[39m, \u001b[32m4\u001b[39m), (\u001b[32m\"Bob\"\u001b[39m, \u001b[32m2\u001b[39m))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv.reduceByKey(_ + _ ).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to sort the k-v RDD by key, we can use `sortByKey()`. If we wish to sort by value, we can use `sortBy()` and specify a lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres70\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, \u001b[32m5\u001b[39m),\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m\"Bob\"\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, \u001b[32m3\u001b[39m),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, \u001b[32m1\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres71\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, \u001b[32m5\u001b[39m),\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m\"Bob\"\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, \u001b[32m3\u001b[39m),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, \u001b[32m1\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv.sortBy(_._1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to create a collection of keys, we use `keys()`, if we want the values, we use `values()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres73\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"Alice\"\u001b[39m, \u001b[32m\"Bob\"\u001b[39m, \u001b[32m\"Charlie\"\u001b[39m, \u001b[32m\"Alice\"\u001b[39m, \u001b[32m\"Charlie\"\u001b[39m)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv.keys.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres74\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m5\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv.values.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we demonstrate the `mapValues()`/`flatMapValues()` , which works in the same way as map on all values of the k-v RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres79_0\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, \u001b[32m10\u001b[39m),\n",
       "  (\u001b[32m\"Bob\"\u001b[39m, \u001b[32m4\u001b[39m),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, \u001b[32m6\u001b[39m),\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, \u001b[32m4\u001b[39m),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, \u001b[32m2\u001b[39m)\n",
       ")\n",
       "\u001b[36mres79_1\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, \u001b[32m10\u001b[39m),\n",
       "  (\u001b[32m\"Bob\"\u001b[39m, \u001b[32m4\u001b[39m),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, \u001b[32m6\u001b[39m),\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, \u001b[32m4\u001b[39m),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, \u001b[32m2\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv.map( e => (e._1, e._2 * 2) ) .collect()\n",
    "kv.mapValues(_ * 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres76\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, (\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m))] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, (\u001b[32m5\u001b[39m, \u001b[32m10\u001b[39m)),\n",
       "  (\u001b[32m\"Bob\"\u001b[39m, (\u001b[32m2\u001b[39m, \u001b[32m4\u001b[39m)),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, (\u001b[32m3\u001b[39m, \u001b[32m6\u001b[39m)),\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, (\u001b[32m2\u001b[39m, \u001b[32m4\u001b[39m)),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, (\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m))\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv.mapValues(x => (x, x*2)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres77\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, \u001b[32m5\u001b[39m),\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, \u001b[32m10\u001b[39m),\n",
       "  (\u001b[32m\"Bob\"\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m\"Bob\"\u001b[39m, \u001b[32m4\u001b[39m),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, \u001b[32m3\u001b[39m),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, \u001b[32m6\u001b[39m),\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, \u001b[32m4\u001b[39m),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, \u001b[32m2\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv.flatMapValues(x => Array(x, x*2)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mItem\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mItem\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mItem\u001b[39m(\u001b[32m\"ToyA\"\u001b[39m, \u001b[32m11.0\u001b[39m, \u001b[32m\"Electrical\"\u001b[39m),\n",
       "  \u001b[33mItem\u001b[39m(\u001b[32m\"ToyB\"\u001b[39m, \u001b[32m12.0\u001b[39m, \u001b[32m\"Electrical\"\u001b[39m),\n",
       "  \u001b[33mItem\u001b[39m(\u001b[32m\"ToyC\"\u001b[39m, \u001b[32m100.0\u001b[39m, \u001b[32m\"Electrical\"\u001b[39m),\n",
       "  \u001b[33mItem\u001b[39m(\u001b[32m\"ToyD\"\u001b[39m, \u001b[32m110.0\u001b[39m, \u001b[32m\"Electrical\"\u001b[39m),\n",
       "  \u001b[33mItem\u001b[39m(\u001b[32m\"ChocA\"\u001b[39m, \u001b[32m2.1\u001b[39m, \u001b[32m\"Food\"\u001b[39m),\n",
       "  \u001b[33mItem\u001b[39m(\u001b[32m\"ChocB\"\u001b[39m, \u001b[32m2.12\u001b[39m, \u001b[32m\"Food\"\u001b[39m),\n",
       "  \u001b[33mItem\u001b[39m(\u001b[32m\"ChocC\"\u001b[39m, \u001b[32m10.0\u001b[39m, \u001b[32m\"Food\"\u001b[39m),\n",
       "  \u001b[33mItem\u001b[39m(\u001b[32m\"ChocD\"\u001b[39m, \u001b[32m11.0\u001b[39m, \u001b[32m\"Food\"\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Item(val name: String, val price: Double, val category: String)\n",
    "\n",
    "val data = Vector(\n",
    "    Item(\"ToyA\", 11, \"Electrical\"),\n",
    "    Item(\"ToyB\", 12, \"Electrical\"),\n",
    "    Item(\"ToyC\", 100, \"Electrical\"),\n",
    "    Item(\"ToyD\", 110, \"Electrical\"),\n",
    "    Item(\"ChocA\", 2.1, \"Food\"),\n",
    "    Item(\"ChocB\", 2.12, \"Food\"),\n",
    "    Item(\"ChocC\", 10.0, \"Food\"),\n",
    "    Item(\"ChocD\", 11.0, \"Food\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres85\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mItem\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mItem\u001b[39m(\u001b[32m\"ToyA\"\u001b[39m, \u001b[32m11.0\u001b[39m, \u001b[32m\"Electrical\"\u001b[39m),\n",
       "  \u001b[33mItem\u001b[39m(\u001b[32m\"ToyB\"\u001b[39m, \u001b[32m12.0\u001b[39m, \u001b[32m\"Electrical\"\u001b[39m),\n",
       "  \u001b[33mItem\u001b[39m(\u001b[32m\"ToyC\"\u001b[39m, \u001b[32m100.0\u001b[39m, \u001b[32m\"Electrical\"\u001b[39m),\n",
       "  \u001b[33mItem\u001b[39m(\u001b[32m\"ToyD\"\u001b[39m, \u001b[32m110.0\u001b[39m, \u001b[32m\"Electrical\"\u001b[39m),\n",
       "  \u001b[33mItem\u001b[39m(\u001b[32m\"ChocA\"\u001b[39m, \u001b[32m2.1\u001b[39m, \u001b[32m\"Food\"\u001b[39m),\n",
       "  \u001b[33mItem\u001b[39m(\u001b[32m\"ChocB\"\u001b[39m, \u001b[32m2.12\u001b[39m, \u001b[32m\"Food\"\u001b[39m),\n",
       "  \u001b[33mItem\u001b[39m(\u001b[32m\"ChocC\"\u001b[39m, \u001b[32m10.0\u001b[39m, \u001b[32m\"Food\"\u001b[39m),\n",
       "  \u001b[33mItem\u001b[39m(\u001b[32m\"ChocD\"\u001b[39m, \u001b[32m11.0\u001b[39m, \u001b[32m\"Food\"\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmyData\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mItem\u001b[39m] = ParallelCollectionRDD[79] at parallelize at cmd87.sc:1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myData = sc.parallelize(data) // data.toRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres89_0\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"ToyA\"\u001b[39m,\n",
       "  \u001b[32m\"ToyB\"\u001b[39m,\n",
       "  \u001b[32m\"ToyC\"\u001b[39m,\n",
       "  \u001b[32m\"ToyD\"\u001b[39m,\n",
       "  \u001b[32m\"ChocA\"\u001b[39m,\n",
       "  \u001b[32m\"ChocB\"\u001b[39m,\n",
       "  \u001b[32m\"ChocC\"\u001b[39m,\n",
       "  \u001b[32m\"ChocD\"\u001b[39m\n",
       ")\n",
       "\u001b[36mres89_1\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m11.0\u001b[39m, \u001b[32m12.0\u001b[39m, \u001b[32m100.0\u001b[39m, \u001b[32m110.0\u001b[39m, \u001b[32m2.1\u001b[39m, \u001b[32m2.12\u001b[39m, \u001b[32m10.0\u001b[39m, \u001b[32m11.0\u001b[39m)\n",
       "\u001b[36mres89_2\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m\"ToyA\"\u001b[39m, \u001b[32m\"Electrical\"\u001b[39m),\n",
       "  (\u001b[32m\"ToyB\"\u001b[39m, \u001b[32m\"Electrical\"\u001b[39m),\n",
       "  (\u001b[32m\"ToyC\"\u001b[39m, \u001b[32m\"Electrical\"\u001b[39m),\n",
       "  (\u001b[32m\"ToyD\"\u001b[39m, \u001b[32m\"Electrical\"\u001b[39m),\n",
       "  (\u001b[32m\"ChocA\"\u001b[39m, \u001b[32m\"Food\"\u001b[39m),\n",
       "  (\u001b[32m\"ChocB\"\u001b[39m, \u001b[32m\"Food\"\u001b[39m),\n",
       "  (\u001b[32m\"ChocC\"\u001b[39m, \u001b[32m\"Food\"\u001b[39m),\n",
       "  (\u001b[32m\"ChocD\"\u001b[39m, \u001b[32m\"Food\"\u001b[39m)\n",
       ")\n",
       "\u001b[36mres89_3\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"ToyA\"\u001b[39m, \u001b[32m\"ToyB\"\u001b[39m, \u001b[32m\"ToyC\"\u001b[39m, \u001b[32m\"ToyD\"\u001b[39m, \u001b[32m\"ChocD\"\u001b[39m)\n",
       "\u001b[36mres89_4\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mDouble\u001b[39m)] = \u001b[33mArray\u001b[39m((\u001b[32m\"Food\"\u001b[39m, \u001b[32m25.22\u001b[39m), (\u001b[32m\"Electrical\"\u001b[39m, \u001b[32m233.0\u001b[39m))\n",
       "\u001b[36mres89_5\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m2.1\u001b[39m\n",
       "\u001b[36mres89_6\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m110.0\u001b[39m\n",
       "\u001b[36mres89_7\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m32.2775\u001b[39m"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myData.map(_.name).collect()\n",
    "myData.map(_.price).collect()\n",
    "myData.map(e => (e.name, e.category)).collect()\n",
    "\n",
    "myData.filter(_.price > 10).map(_.name).collect()\n",
    "myData.map(e => (e.category, e.price)).reduceByKey(_ +_).collect()\n",
    "\n",
    "myData.map(_.price).min()\n",
    "myData.map(_.price).max()\n",
    "myData.map(_.price).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "### Part 1: RDDs from Collections\n",
    "\n",
    "* start with a vector of Items\n",
    "    * (eg., create a \n",
    "        `case class Item(val name: String, val price: Double, val category: String)`)\n",
    "        \n",
    "    * with lots of different values\n",
    "    \n",
    "* then parrlllize your vector\n",
    "    * ie., val myRDD = sc.parr...\n",
    "    \n",
    "* query\n",
    "    * select all: (HINT .map .collect)\n",
    "        * name\n",
    "        * name and category (HINT: tuple)\n",
    "        * price\n",
    "        \n",
    "    * select name of all expensive items\n",
    "        * HINT: filter, map, collect\n",
    "        \n",
    "    * select all  (category, price) THEN reduceByKey to sum on price\n",
    "    \n",
    "    * calculate: min, mean, max of price\n",
    "        * HINT: project to .price first\n",
    "        \n",
    "        \n",
    "### Part 2: RDDs from files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* given the following `data`\n",
    "    * select the top(5) passenger_counts\n",
    "    * HINT: map then top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m]] = MapPartitionsRDD[118] at map at cmd95.sc:1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = sc.textFile(\"test.csv\").map { line => line.split(\",\") }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util._\u001b[39m"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres106\u001b[39m: \u001b[32mBoolean\u001b[39m = false"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Try(\"sd\".toInt).isSuccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres109\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m6\u001b[39m, \u001b[32m6\u001b[39m, \u001b[32m6\u001b[39m, \u001b[32m6\u001b[39m, \u001b[32m6\u001b[39m)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.filter( \n",
    "    line => Try(line(3).toInt).isSuccess \n",
    ").map { \n",
    "    line => line(3).toInt \n",
    "}.top(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark allows one to create DataFrames similar to once provided by Pandas, and allows SQL to be performed on them.  And of course everything, the DataFrame, and the SQL operations are stored and performed on the distributed cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLContext\n",
    "\n",
    "To use Spark SQL features, we need a dedicated SQL context handler. This serves as the point of call for all SQL related operations, and can be instantiated using the Spark context we already have.\n",
    "\n",
    "An `SQLContext` can also be created directly from a SparkSession -- which is the preffered method for the newer versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame from a SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside: Spark uses long chains of method calls which, for clarity, are conventionally placed on their own lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL provdies `spark.read` which will parse and distribute your data if its in one of its supported formats (eg., csv, json, parquet, etc.). \n",
    "\n",
    "If your file contains its own schema (eg., a csv with a header row), spark can use it to structure your data appropriately. If it does not you can manually define one, as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "import org.apache.spark.sql.types.Metadata\n",
    "\n",
    "val schema_item = StructType(Array(\n",
    "  StructField(\"UserID\", LongType),\n",
    "  StructField(\"Title\", StringType),\n",
    "))\n",
    "\n",
    "val dfi = (\n",
    "  spark\n",
    "  .read\n",
    "  .schema(schema_item)\n",
    "  .option(\"header\", \"false\")\n",
    "  .option(\"delimiter\", \"|\")\n",
    "  .csv(\"etc/ml-100k/u.item\")\n",
    ")\n",
    "\n",
    "dfi.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val df = (\n",
    "  spark\n",
    "  .read\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .csv(\"etc/responses.csv\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select(\"Music\", \"Internet\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns.slice(40, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select(\"Music\", \"Internet\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "df\n",
    "    .select(\"Music\", \"Internet\")\n",
    "    .filter($\"Music\" > 2)\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{desc, asc}\n",
    "\n",
    "df\n",
    "    .select(\"Music\", \"Internet\")\n",
    "    .filter($\"Music\" > 2)\n",
    "    .orderBy(desc(\"Internet\"))\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df\n",
    "    .select(\"Music\", \"Internet\", \"Physics\", \"Religion\")\n",
    "    .filter($\"Music\" > 2)\n",
    "    .groupBy($\"Religion\")\n",
    "    .mean()\n",
    "    .orderBy($\"Religion\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    SELECT AVG(Physics), STDDEV(Religion), COUNT(Religion)\n",
    "    FROM results WHERE Music > 2 \n",
    "    GROUP BY Physics, Religion\n",
    "    WHERE Religion >0\n",
    "    ORDER BY Religion ASC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val results = ( \n",
    "    df\n",
    "    .select(\"Physics\", \"Religion\")\n",
    "    .filter($\"Music\" > 2)\n",
    "    .groupBy(\"Religion\")\n",
    "    .agg(\n",
    "        mean(\"Physics\").alias(\"Epx\"),\n",
    "        stddev(\"Physics\").alias(\"Spx\"),\n",
    "        count(\"Religion\").alias(\"Nr\")\n",
    "    )\n",
    "    .filter($\"Religion\" > 0)\n",
    "    .orderBy(\"Religion\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val x = results.select(\"Religion\").as[Int].collect().toSeq\n",
    "val y = results.select(\"Epx\").as[Double].collect().toSeq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(Seq(Bar(x, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.element._, plotly.layout._\n",
    "\n",
    "lazy val layout = Layout(\n",
    "  title = \"R vs P\"\n",
    ")\n",
    "\n",
    "plot(Seq(Scatter(x,y, mode = ScatterMode(ScatterMode.Markers))), layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, but not least, we stop the SparkContext object, much in the same way we would close a connection to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
